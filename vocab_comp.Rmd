---
title: "Cross-Linguistic Comparison of Developmental Trajectories in Vocabulary Composition"
author: "Mika Braginsky, Daniel Yurovsky, Virginia Marchman, and Michael Frank"
date: "2015-04-30"
output:
  html_document:
    highlight: tango
    theme: spacelab
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE, message=FALSE, warning=FALSE)
font = "Open Sans"
```

Load required libraries.
```{r libraries, cache=FALSE}
library(ggplot2)
library(grid)
library(dplyr)
library(tidyr)
library(magrittr)
library(quadprog)
library(bootstrap)
source('data_loading.R')
```

Load in Wordbank common data.
```{r database}
wordbank <- connect.to.wordbank("prod")

common.tables <- get.common.tables(wordbank)

items <- get.item.data(common.tables$wordmapping,
                       common.tables$instrumentsmap,
                       common.tables$category) %>%
  filter(form == "WS", type == "word")

instrument.tables <- get.instrument.tables(wordbank, common.tables$instruments)
languages <- unique(instrument.tables$language)
```

Get vocabulary composition data for all languages.
```{r vocab_composition}
get.vocab.composition <- function(lang) {
  
  lang.vocab.items <- filter(items, language == lang) %>%
    filter(lexical_category != "other") %>%
    rename(column = item.id) %>%
    mutate(item.id = as.numeric(substr(column, 6, nchar(column))))
  
  lang.instrument.table <- filter(instrument.tables, language == lang,
                                  form == "WS")$table[[1]]
  
  lang.vocab.data <- get.instrument.data(lang.instrument.table,
                                         lang.vocab.items$column) %>%
    left_join(select(lang.vocab.items, item.id, lexical_category, item, definition)) %>%
    mutate(value = ifelse(is.na(value), "", value),
           value = value == "produces")
  
  num.words <- nrow(lang.vocab.items)
  
  lang.vocab.summary <- lang.vocab.data %>%
    group_by(data_id, lexical_category) %>%
    summarise(sum = sum(value),
              diff = length(value) - sum,
              mean = sum / length(value))
  
  lang.vocab.sizes <- lang.vocab.summary %>%
    summarise(vocab.mean = sum(sum) / num.words)
  
  left_join(lang.vocab.summary, lang.vocab.sizes) %>%
    mutate(language = lang)
  
  }

vocab.composition <- bind_rows(sapply(languages, get.vocab.composition,
                                      simplify = FALSE)) %>%
  filter(lexical_category != "unknown",
         lexical_category != "other") %>%
  mutate(lexical_category = factor(lexical_category,
                                   levels=c("nouns", "predicates", "function_words"),
                                   labels=c("Nouns", "Predicates", "Function Words")))
```

Base plot for looking at vocabulary composition.
```{r base_plot}
base_plot <- ggplot(vocab.composition,
                    aes(x = vocab.mean, y = mean, colour = lexical_category)) +
  facet_wrap(~ language) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                     name = "Proportion of Category\n") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                     name = "\nVocabulary Size") +
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  theme_bw(base_size = 12) + 
  theme(legend.position = c(0.068, 0.95),
        legend.text = element_text(size = 9),
        legend.title = element_text(size = 9, lineheight = unit(0.8, "char")),
        legend.key.height = unit(0.8, "char"),
        legend.key.width = unit(0.3, "cm"),
        legend.key = element_blank(),
        legend.background = element_rect(fill = "transparent"))+
      #  text = element_text(family = font)) +
  scale_color_brewer(palette = "Set1", name = "Lexical Category")
```

Plot vocabulary composition as a function of vocabulary size for each language.
```{r plot_points, fig.width=10, fig.height=10}
base_plot + geom_jitter(size = 0.7)
```

Make an S3 class for a clm (constrained linear model), which uses quadratic programming to run a regression on data with a specified formula, subject to the constraint that the coefficients of the regression sum to 1 (in the future could support arbitrary constraints on the coefficients).
```{r}
clm <- function(formula, data, ...) {
  M <- model.frame(formula, data)
  y <- M[,1]
  X <- as.matrix(M[,-1])
  s <- solve.QP(t(X) %*% X, t(y) %*% X, matrix(1, nr=ncol(X), nc=1), 1, meq=1)
  class(s) <- "clm"
  s$formula <- formula
  return(s)
  }

# S3 predict method for clm
predict.clm <- function(object, newdata) {
  M <- as.matrix(model.frame(object$formula[-2], newdata))
  s <- object$solution
  p <- (M %*% s)
  rownames(p) <- NULL
  p[,1]
  }

# S3 predictdf method for clm (called by stat_smooth)
predictdf.clm <- function(model, xseq, se, level) {
  pred <- predict(model, newdata = data.frame(x = xseq))
  data.frame(x = xseq, y = as.vector(pred))
  }
```

Plot vocabulary composition as a function of vocabulary size for each language with cubic contrained lm curves.
```{r plot_models, fig.width=10, fig.height=10}
base_plot + geom_smooth(method = "clm", formula = y ~ I(x^3) + I(x^2) + x - 1)
```

Resample kids 
```{r}
sample.areas <- function(d,num.times=1000) {
  
  poly.area <- function(group.data) {
    model = clm(mean ~ I(vocab.mean^3) + I(vocab.mean^2) + vocab.mean - 1, 
                     data = group.data)
    return((model$solution %*% c(1/4,1/3,1/2) - .5)[1])
  }
  
  counter = 1
  sample.area <- function(d) {
    d.frame <- d %>%
      group_by(language) %>%
      sample_frac(replace = TRUE) %>% #resample kids
      group_by(language,lexical_category) %>%
      do(area = poly.area(.)) %>%
      mutate(area = area[1]) %>%
      rename_(.dots = setNames("area",counter))

    counter <<- counter + 1 #Increment counter outside scope
    return(d.frame)
  }

  areas <- replicate(num.times, sample.area(d),simplify=FALSE)
  
  Reduce(left_join,areas) %>%
    gather(sample,area,-language, -lexical_category)
}
```

```{r}

areas <- sample.areas(vocab.composition,1000)

area.summary <- areas %>%
  group_by(language, lexical_category) %>%
  summarise(mean =  mean(area),
            ci.high = quantile(area,.975),
            ci.low = quantile(area,.025))

nouns <- filter(area.summary, lexical_category == "Nouns")
noun.levels <- nouns$language[order(nouns$mean, nouns$language, decreasing = FALSE)]
area.summary %<>% ungroup() %>%mutate(language = factor(language, levels = noun.levels))
```

For each language and lexical category, estimate the area between the proportion curve predicted by the model and the diagonal.
```{r}

clm.dists <- vocab.composition %>%
  group_by(language, lexical_category)

width <- 0.001 # can get CI arbitrarily small by lowering this???
pts <- seq(0, 1, width)

get_area <- function(predictions, pts, width) {
  dists <- predictions - pts
  first_dists <- dists[1:length(dists)-1]
  second_dists <- dists[2:length(dists)]
  mid_dists <- (first_dists + second_dists) / 2
  sum(mid_dists * width)
  }

get_area_samples <- function(data_grp) {
  model <- clm(mean ~ I(vocab.mean^3) + I(vocab.mean^2) + vocab.mean - 1, data = data_grp)
  predictions <- predict(model, data.frame(vocab.mean = pts))
  samples <- bootstrap(predictions, 1000, function(x) get_area(x, pts, width))
  samples$thetastar
  }

clm.dists <- vocab.composition %>%
  group_by(language, lexical_category) %>%
  #   do(samples = bootstrap(predict(clm(mean ~ I(vocab.mean^3) + I(vocab.mean^2) + vocab.mean - 1,
  #                                           data = .),
  #                                       data.frame(vocab.mean = pts)),
  #                               1000, function(x) get_area(x, pts, width))$thetastar) %>%
  do(samples = get_area_samples(.)) %>%
  mutate(mean = mean(samples),
         cil = quantile(samples, 0.025),
         cih = quantile(samples, 0.975))

nouns <- filter(clm.dists, lexical_category == "Nouns")
noun.levels <- nouns$language[order(nouns$mean, nouns$language, decreasing = FALSE)]
clm.dists %<>% mutate(language = factor(language, levels = noun.levels))
```

Plot each language's area estimate by lexical category.
```{r dists_plot, fig.width=8, fig.height=8}

ggplot(area.summary, aes(y = language, x = mean, col = lexical_category)) +
  facet_grid(lexical_category ~ .) +
  geom_point() +
  geom_segment(aes(x = ci.low, xend = ci.high,
                   y = language, yend = language)) +
  geom_hline(aes(y = 0), color = "gray", linetype = "dashed") +
  scale_colour_brewer(palette = "Set1", name = "", guide=FALSE) +
  theme_bw() +
 # theme(text = element_text(family = font)) + 
  geom_vline(xintercept=0, lty=2) + 
  ylab("") +
  xlab("\nRelative representation in early vocabulary") 
```